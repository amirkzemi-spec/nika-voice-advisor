import os
from dotenv import load_dotenv
from openai import AsyncOpenAI
from rag.retriever import get_context_for_query  # âœ… RAG
from utils.session_memory import summarize_memory, save_session, get_session  # ğŸ§  Memory integration
from utils.advisor_logic import detect_mode, get_or_ask_profile  # ğŸ¯ Advisory logic

# -------------------------------------------
# ğŸ§© Simple internal logger (no dependencies)
# -------------------------------------------
def log(tag: str, message: str, level: str = "info"):
    color_map = {
        "info": "\033[94m", "warn": "\033[93m",
        "error": "\033[91m", "success": "\033[92m"
    }
    color = color_map.get(level, "\033[0m")
    reset = "\033[0m"
    print(f"{color}[{tag}] {message}{reset}")

# ğŸ” Load API key and init OpenAI client
load_dotenv()
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))



# ----------------------------------------------------
# ğŸ¯ Generate advisory recommendation
# ----------------------------------------------------
async def generate_advice(user_id: str, is_farsi: bool = True) -> str:
    """
    Once profile is complete â†’ generate personalized recommendations
    based on user's answers.
    """
    from utils.session_memory import get_session
    session = await get_session(user_id)
    profile = session.get("profile", {})

    profile_summary = "\n".join([f"{k}: {v}" for k, v in profile.items()])
    log("ğŸ§© Advisory", f"Generating recommendations for: {profile_summary}")

    prompt = f"""
User profile:
{profile_summary}

You are Nika, an expert immigration advisor.
Recommend the top 2 countries and study programs suitable for this user.
Be practical and concise (max 80 words).
Respond in the same language as the user.
    """

    try:
        completion = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Expert immigration advisor"},
                {"role": "user", "content": prompt.strip()},
            ],
            temperature=0.6,
            max_tokens=150,
        )

        reply = completion.choices[0].message.content.strip()
        await save_session(user_id, "advisory_recommendation", "profile_complete", reply)
        log("ğŸ¤– Advisor Reply", reply)
        return reply

    except Exception as e:
        log("âŒ Advisory", f"Error generating recommendation: {e}", level="error")
        return (
            "Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø±Ø® Ø¯Ø§Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."
            if is_farsi else
            "There was an error generating your recommendation. Please try again."
        )

# ----------------------------------------------------
# ğŸ§  GPT reply with Mode Switch + RAG + Memory + Smart Tone
# ----------------------------------------------------
async def gpt_reply(user_text: str, user_id: str = "web_user", intent: str = "unknown") -> str:
    """
    Handles two flows:
    1ï¸âƒ£ General Q&A mode (RAG + memory)
    2ï¸âƒ£ Advisory mode (profile guidance and personalized suggestions)
    Adds human-like tone and summarization for multi-question inputs.
    """

    if not user_text or not user_text.strip():
        return "âš ï¸ Ù…Ù† ØµØ¯Ø§ÛŒ ÙˆØ§Ø¶Ø­ÛŒ Ù†Ø´Ù†ÛŒØ¯Ù…. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ú¯Ùˆ."

    # ğŸˆ¯ Detect Farsi vs English
    is_farsi = any("\u0600" <= ch <= "\u06FF" for ch in user_text)

    # ğŸ§Š Detect first-time user
    session = await get_session(user_id)
    if not session:
        log("ğŸ‘‹ Welcome", "First interaction detected â€” sending greeting.")
        await save_session(user_id, "intro", "first_greeting", "done")
        return (
            "Hi there! Welcome to Nika Visa AI Assistant. "
            "Would you like to ask general immigration questions, "
            "or would you like me to give personalized advice based on your background?"
            if not is_farsi
            else
            "Ø³Ù„Ø§Ù…! Ø®ÙˆØ´ Ø§ÙˆÙ…Ø¯ÛŒ Ø¨Ù‡ Ù†ÛŒÚ©Ø§ ÙˆÛŒØ²Ø§. "
            "Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ù…Ù‡Ø§Ø¬Ø±ØªÛŒ Ø¨Ù¾Ø±Ø³ÛŒ ÛŒØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø±Ø§ÛŒØ· Ø®ÙˆØ¯Øª Ø¨Ø±Ø§Øª Ù…Ø´Ø§ÙˆØ±Ù‡ Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø¨Ø¯Ù…ØŸ"
        )

    # ğŸ¯ Detect user mode (advisory vs general)
    mode = await detect_mode(user_text, user_id)
    log("ğŸ§­ Mode", f"Active mode: {mode}")

    # ğŸ‘¤ If advisory mode â†’ collect or complete profile
    if mode == "advisory":
        profile, question = await get_or_ask_profile(user_id)
        if question:
            return question  # Ask next missing field before GPT
        else:
            log("ğŸ§¾ Profile", f"Profile complete: {profile}")

    # ğŸ§  Retrieve past memory summary
    try:
        memory_context = await summarize_memory(user_id)
    except Exception:
        memory_context = ""

    # ğŸ” Retrieve RAG context
    try:
        context = get_context_for_query(user_text)
    except Exception:
        context = ""

    # ğŸ’¬ Smart handling for multi-question messages
    question_count = user_text.count("?") + user_text.count("ØŸ")
    too_many_questions = question_count >= 3

    if too_many_questions:
        log("ğŸ§® Query", f"Detected {question_count} questions â€” summarizing mode.")
        if is_farsi:
            polite_intro = (
                "Ú†Ù†Ø¯ Ø³Ø¤Ø§Ù„ Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨ Ù¾Ø±Ø³ÛŒØ¯ÛŒ! Ø§Ø¬Ø§Ø²Ù‡ Ø¨Ø¯Ù‡ Ø§Ø² Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ†Ø´ Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒÙ… "
                "Ùˆ Ø¨Ø¹Ø¯ Ø¯ÙˆÙ†Ù‡â€ŒØ¯ÙˆÙ†Ù‡ Ø¨Ù‚ÛŒÙ‡ Ø±Ùˆ Ù‡Ù… Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ…."
            )
            pre_prompt = (
                "Ú©Ø§Ø±Ø¨Ø± Ú†Ù†Ø¯ Ø³Ø¤Ø§Ù„ Ù¾Ø´Øª Ø³Ø± Ù‡Ù… Ù¾Ø±Ø³ÛŒØ¯Ù‡ Ø§Ø³Øª. "
                "Ø¨Ø§ Ù„Ø­Ù†ÛŒ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…Ø´Ø§ÙˆØ±â€ŒÚ¯ÙˆÙ†Ù‡ ÙÙ‚Ø· Ø¨Ù‡ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø³Ø¤Ø§Ù„Ø§Øª Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡ "
                "Ùˆ Ø¨Ú¯Ùˆ Ú©Ù‡ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒ Ø¨Ù‚ÛŒÙ‡ Ø±Ø§ Ù‡Ù… ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒ."
            )
        else:
            polite_intro = (
                "Thatâ€™s a lot of great questions! Letâ€™s start with the most important one, "
                "and Iâ€™ll help you go through the rest next."
            )
            pre_prompt = (
                "The user asked several questions in one message. "
                "You are a calm, friendly immigration consultant â€” acknowledge it politely, "
                "answer the key questions first, and mention you can cover others next."
            )
    else:
        pre_prompt = ""
        polite_intro = ""

    # ğŸ§© Build system prompt (consultant tone)
    system_prompt = (
        "ØªÙˆ Ù†ÛŒÚ©Ø§ Ù‡Ø³ØªÛŒØŒ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù…Ù‡Ø§Ø¬Ø±Øª Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ. "
        "Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒØª Ø¨Ø§ÛŒØ¯ Ú©ÙˆØªØ§Ù‡ØŒ ØµÙˆØªÛŒâ€ŒÙ¾Ø³Ù†Ø¯ Ùˆ Ù…ÙˆØ¯Ø¨Ø§Ù†Ù‡ Ø¨Ø§Ø´Ù†Ø¯. "
        f"{pre_prompt}\n{context}\n{memory_context}"
        if is_farsi else
        "You are Nika, a friendly and accurate visa & immigration consultant. "
        "Keep replies short, warm, and natural for spoken output. "
        f"{pre_prompt}\nUse this info if relevant:\n{context}\n{memory_context}"
    )

    # ğŸ§  Dynamic length control
    max_len = 180 if too_many_questions else 100

    # ğŸš€ Generate GPT reply
    try:
        completion = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_text.strip()},
            ],
            temperature=0.45,
            max_tokens=max_len,
        )
        reply = completion.choices[0].message.content.strip()

        if too_many_questions:
            reply = f"{polite_intro}\n{reply}"

        log("ğŸ¤– GPT Reply", reply)
        await save_session(user_id, intent, user_text, reply)
        return reply

    except Exception as e:
        log("âŒ GPT", f"Error: {e}", level="error")
        return (
            "Ù…ØªØ§Ø³ÙÙ…ØŒ Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†."
            if is_farsi else
            "Sorry, something went wrong. Please try again."
        )

# ----------------------------------------------------
# ğŸ”Š Quick GPT â†’ TTS helper
# ----------------------------------------------------
async def text_to_voice(user_text: str, out_path: str):
    from utils.text_to_speech import speak_reply
    reply = await gpt_reply(user_text)
    await speak_reply(reply, out_path)
    return out_path
